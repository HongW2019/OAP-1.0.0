<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://hongw2019.github.io/OAP-1.0.0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/OAP-1.0.0/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/OAP-1.0.0/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/OAP-1.0.0/css/style.css">

  
    
<link rel="stylesheet" href="/OAP-1.0.0/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/OAP-1.0.0/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/OAP-1.0.0/">Home</a>
        
          <a class="main-nav-link" href="/OAP-1.0.0/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/OAP-1.0.0/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://hongw2019.github.io/OAP-1.0.0"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Developer-Guide" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/OAP-1.0.0/2020/12/17/Developer-Guide/" class="article-date">
  <time class="dt-published" datetime="2020-12-17T14:32:41.506Z" itemprop="datePublished">2020-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Developer-Guide"><a href="#Developer-Guide" class="headerlink" title="Developer Guide"></a>Developer Guide</h1><p>This document is a supplement to the whole <a href="../../../docs/Developer-Guide.md">Developer Guide</a> for SQL Index and Data Source Cache.<br>After following that document, you can continue more details for SQL Index and Data Source Cache.</p>
<ul>
<li><a href="#Building">Building</a></li>
<li><a href="#enabling-numa-binding-for-pmem-in-spark">Enabling NUMA binding for Intel® Optane™ DC Persistent Memory in Spark</a></li>
</ul>
<h2 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h2><h3 id="Building-SQL-Index-and-Data-Source-Cache"><a href="#Building-SQL-Index-and-Data-Source-Cache" class="headerlink" title="Building SQL Index and  Data Source Cache"></a>Building SQL Index and  Data Source Cache</h3><p>Building with <a target="_blank" rel="noopener" href="http://maven.apache.org/">Apache Maven*</a>.</p>
<p>Clone the OAP project:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone -b &lt;tag-version&gt;  https:&#x2F;&#x2F;github.com&#x2F;Intel-bigdata&#x2F;OAP.git</span><br><span class="line">cd OAP</span><br></pre></td></tr></table></figure>

<p>Build the <code>oap-cache</code> package:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean -pl com.intel.oap:oap-cache -am package</span><br></pre></td></tr></table></figure>

<h3 id="Running-Tests"><a href="#Running-Tests" class="headerlink" title="Running Tests"></a>Running Tests</h3><p>Run all the tests:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean -pl com.intel.oap:oap-cache -am test</span><br></pre></td></tr></table></figure>
<p>Run a specific test suite, for example <code>OapDDLSuite</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn -pl com.intel.oap:oap-cache -am -DwildcardSuites&#x3D;org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test</span><br></pre></td></tr></table></figure>
<p><strong>NOTE</strong>: Log level of unit tests currently default to ERROR, please override oap-cache/oap/src/test/resources/log4j.properties if needed.</p>
<h3 id="Building-with-Intel®-Optane™-DC-Persistent-Memory-Module"><a href="#Building-with-Intel®-Optane™-DC-Persistent-Memory-Module" class="headerlink" title="Building with Intel® Optane™ DC Persistent Memory Module"></a>Building with Intel® Optane™ DC Persistent Memory Module</h3><h4 id="Prerequisites-for-building-with-PMem-support"><a href="#Prerequisites-for-building-with-PMem-support" class="headerlink" title="Prerequisites for building with PMem support"></a>Prerequisites for building with PMem support</h4><p>Install the required packages on the build system:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://help.directadmin.com/item.php?id=494">cmake</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/memkind/memkind/tree/v1.10.1">memkind</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/pmem/vmemcache">vmemcache</a></li>
<li><a target="_blank" rel="noopener" href="http://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/">Plasma</a></li>
</ul>
<h4 id="memkind-installation"><a href="#memkind-installation" class="headerlink" title="memkind installation"></a>memkind installation</h4><p>The memkind library depends on <code>libnuma</code> at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">git clone -b v1.10.1 https:&#x2F;&#x2F;github.com&#x2F;memkind&#x2F;memkind</span><br><span class="line">cd memkind</span><br><span class="line">.&#x2F;autogen.sh</span><br><span class="line">.&#x2F;configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">&#96;&#96;&#96; </span><br><span class="line">#### vmemcache installation</span><br><span class="line"></span><br><span class="line">To build vmemcache library from source, you can (for RPM-based linux as example):</span><br></pre></td></tr></table></figure>
<p>git clone <a target="_blank" rel="noopener" href="https://github.com/pmem/vmemcache">https://github.com/pmem/vmemcache</a><br>cd vmemcache<br>mkdir build<br>cd build<br>cmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCPACK_GENERATOR=rpm<br>make package<br>sudo rpm -i libvmemcache*.rpm</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#### Plasma installation</span><br><span class="line"></span><br><span class="line">To use optimized Plasma cache with OAP, you need following components:  </span><br><span class="line"></span><br><span class="line">   (1) &#96;libarrow.so&#96;, &#96;libplasma.so&#96;, &#96;libplasma_java.so&#96;: dynamic libraries, will be used in Plasma client.   </span><br><span class="line">   (2) &#96;plasma-store-server&#96;: executable file, Plasma cache service.  </span><br><span class="line">   (3) &#96;arrow-plasma-0.17.0.jar&#96;: will be used when compile oap and spark runtime also need it. </span><br><span class="line"></span><br><span class="line">- &#96;.so&#96; file and binary file  </span><br><span class="line">  Clone code from Intel-arrow repo and run following commands, this will install &#96;libplasma.so&#96;, &#96;libarrow.so&#96;, &#96;libplasma_java.so&#96; and &#96;plasma-store-server&#96; to your system path(&#96;&#x2F;usr&#x2F;lib64&#96; by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node.</span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<p>cd /tmp<br>git clone <a target="_blank" rel="noopener" href="https://github.com/Intel-bigdata/arrow.git">https://github.com/Intel-bigdata/arrow.git</a><br>cd arrow &amp;&amp; git checkout branch-0.17.0-oap-1.0<br>cd cpp<br>mkdir release<br>cd release<br>#build libarrow, libplasma, libplasma_java<br>cmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED  ..<br>make -j$(nproc)<br>sudo make install -j$(nproc)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- arrow-plasma-0.17.0.jar  </span><br><span class="line">   &#96;arrow-plasma-0.17.0.jar&#96; is provided in Maven central repo, you can download [it](https:&#x2F;&#x2F;repo1.maven.org&#x2F;maven2&#x2F;com&#x2F;intel&#x2F;arrow&#x2F;arrow-plasma&#x2F;0.17.0&#x2F;arrow-plasma-0.17.0.jar) and copy to &#96;$SPARK_HOME&#x2F;jars&#96; dir.</span><br><span class="line">   </span><br><span class="line">   Or you can manually install it, run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-0.17.0.jar to &#96;$SPARK_HOME&#x2F;jars&#x2F;&#96; dir, cause this jar is needed when using external cache.</span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<p>cd /tmp/arrow/java<br>mvn clean -q -pl plasma -DskipTests install</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#### Building the package</span><br><span class="line">You need to add &#96;-Ppersistent-memory&#96; to build with PMem support. For &#96;noevict&#96; cache strategy, you also need to build with &#96;-Ppersistent-memory&#96; parameter.</span><br></pre></td></tr></table></figure>
<p>mvn clean -q -pl com.intel.oap:oap-cache -am  -Ppersistent-memory -DskipTests package</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">For vmemcache cache strategy, please build with command:</span><br></pre></td></tr></table></figure>
<p>mvn clean -q -pl com.intel.oap:oap-cache -am -Pvmemcache -DskipTests package</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Build with this command to use all of them:</span><br></pre></td></tr></table></figure>
<p>mvn clean -q -pl com.intel.oap:oap-cache -am  -Ppersistent-memory -Pvmemcache -DskipTests package</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## Enabling NUMA binding for PMem in Spark</span><br><span class="line"></span><br><span class="line">### Rebuilding Spark packages with NUMA binding patch </span><br><span class="line"></span><br><span class="line">When using PMem as a cache medium apply the [NUMA](https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;html&#x2F;v4.18&#x2F;vm&#x2F;numa.html) binding patch [numa-binding-spark-3.0.0.patch](.&#x2F;numa-binding-spark-3.0.0.patch) to Spark source code for best performance.</span><br><span class="line"></span><br><span class="line">1. Download src for [Spark-3.0.0](https:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;spark&#x2F;spark-3.0.0&#x2F;spark-3.0.0.tgz) and clone the src from github.</span><br><span class="line"></span><br><span class="line">2. Apply this patch and [rebuild](https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;building-spark.html) the Spark package.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>git apply  numa-binding-spark-3.0.0.patch</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3. Add these configuration items to the Spark configuration file $SPARK_HOME&#x2F;conf&#x2F;spark-defaults.conf to enable NUMA binding.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>spark.yarn.numa.enabled true </p>
<pre><code>**NOTE**: If you are using a customized Spark, you will need to manually resolve the conflicts.

###### \*Other names and brands may be claimed as the property of others.</code></pre>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://hongw2019.github.io/OAP-1.0.0/2020/12/17/Developer-Guide/" data-id="ckisy2kks0000nfpv8l9cfnnn" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Advanced-Configuration" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/OAP-1.0.0/2020/12/17/Advanced-Configuration/" class="article-date">
  <time class="dt-published" datetime="2020-12-17T14:32:41.505Z" itemprop="datePublished">2020-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Advanced-Configuration"><a href="#Advanced-Configuration" class="headerlink" title="Advanced Configuration"></a>Advanced Configuration</h1><p>In addition to usage information provided in <a href="User-Guide.md">User Guide</a>, we provide more strategies for SQL Index and Data Source Cache in this section.</p>
<p>Their needed dependencies like <strong><em>Memkind</em></strong> ,<strong><em>Vmemcache</em></strong> and <strong><em>Plasma</em></strong> can be automatically installed when following <a href="../../../docs/OAP-Installation-Guide.md">OAP Installation Guide</a>, corresponding feature jars can be found under <code>$HOME/miniconda2/envs/oapenv/oap_jars</code>.</p>
<ul>
<li><a href="#Additional-Cache-Strategies">Additional Cache Strategies</a>  In addition to <strong>external</strong> cache strategy, SQL Data Source Cache also supports 3 other cache strategies: <strong>guava</strong>, <strong>noevict</strong>  and <strong>vmemcache</strong>.</li>
<li><a href="#Index-and-Data-Cache-Separation">Index and Data Cache Separation</a>  To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem.</li>
<li><a href="#Cache-Hot-Tables">Cache Hot Tables</a>  Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables.</li>
<li><a href="#Column-Vector-Cache">Column Vector Cache</a>  This document above uses <strong>binary</strong> cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time.</li>
<li><a href="#Large-Scale-and-Heterogeneous-Cluster-Support">Large Scale and Heterogeneous Cluster Support</a> Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters.</li>
</ul>
<h2 id="Additional-Cache-Strategies"><a href="#Additional-Cache-Strategies" class="headerlink" title="Additional Cache Strategies"></a>Additional Cache Strategies</h2><p>Following table shows features of 4 cache strategies on PMem.</p>
<table>
<thead>
<tr>
<th align="left">guava</th>
<th align="left">noevict</th>
<th align="left">vmemcache</th>
<th align="left">external cache</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Use memkind lib to operate on PMem and guava cache strategy when data eviction happens.</td>
<td align="left">Use memkind lib to operate on PMem and doesn’t allow data eviction.</td>
<td align="left">Use vmemcache lib to operate on PMem and LRU cache strategy when data eviction happens.</td>
<td align="left">Use Plasma/dlmalloc to operate on PMem and LRU cache strategy when data eviction happens.</td>
</tr>
<tr>
<td align="left">Need numa patch in Spark for better performance.</td>
<td align="left">Need numa patch in Spark for better performance.</td>
<td align="left">Need numa patch in Spark for better performance.</td>
<td align="left">Doesn’t need numa patch.</td>
</tr>
<tr>
<td align="left">Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.</td>
<td align="left">Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.</td>
<td align="left">Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.</td>
<td align="left">Node-level cache so there are no limitation for executor number.</td>
</tr>
<tr>
<td align="left">Cache data cleaned once executors exited.</td>
<td align="left">Cache data cleaned once executors exited.</td>
<td align="left">Cache data cleaned once executors exited.</td>
<td align="left">No data loss when executors exit thus is friendly to dynamic allocation. But currently it has performance overhead than other cache solutions.</td>
</tr>
</tbody></table>
<ul>
<li><p>For cache solution <code>guava/noevict</code>, make sure <a target="_blank" rel="noopener" href="https://github.com/memkind/memkind/tree/v1.10.1-rc2">Memkind</a> library installed on every cluster worker node. If you have finished <a href="../../../docs/OAP-Installation-Guide.md">OAP Installation Guide</a>, libmemkind will be installed. Or manually build and install it following <a href="./Developer-Guide.md#memkind-installation">memkind-installation</a>, then place <code>libmemkind.so.0</code> under <code>/lib64/</code> on each worker node.</p>
</li>
<li><p>For cache solution <code>vmemcahe/external</code> cache, make sure <a target="_blank" rel="noopener" href="https://github.com/pmem/vmemcache">Vmemcache</a> library has been installed on every cluster worker node. If you have finished <a href="../../../docs/OAP-Installation-Guide.md">OAP Installation Guide</a>, libvmemcache will be installed. Or you can follow the <a href="./Developer-Guide.md#vmemcache-installation">vmemcache-installation</a> steps and make sure <code>libvmemcache.so.0</code> exist under <code>/lib64/</code> directory on each worker node.</p>
</li>
</ul>
<p>If you have followed <a href="../../../docs/OAP-Installation-Guide.md">OAP Installation Guide</a>, <strong><em>Memkind</em></strong> ,<strong><em>Vmemcache</em></strong> and <strong><em>Plasma</em></strong> will be automatically installed.<br>Or you can refer to <a href="../../../docs/Developer-Guide.md">Developer-Guide</a>, there is a shell script to help you install these dependencies automatically.</p>
<h3 id="Use-PMem-Cache"><a href="#Use-PMem-Cache" class="headerlink" title="Use PMem Cache"></a>Use PMem Cache</h3><h4 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h4><p>The following are required to configure OAP to use PMem cache.</p>
<ul>
<li><p>PMem hardware is successfully deployed on each node in cluster.</p>
</li>
<li><p>Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as <code>/mnt/pmem0</code> and <code>/mnt/pmem1</code>. Correctly installed PMem must be formatted and mounted on every cluster worker node. You can follow these commands to destroy interleaved PMem device which you set in <a href="./User-Guide.md#prerequisites-1">User-Guide</a>:</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># destroy interleaved PMem device which you set when using external cache strategy</span><br><span class="line">umount &#x2F;mnt&#x2F;pmem</span><br><span class="line">dmsetup remove striped-pmem</span><br><span class="line">echo y | mkfs.ext4 &#x2F;dev&#x2F;pmem0</span><br><span class="line">echo y | mkfs.ext4 &#x2F;dev&#x2F;pmem1</span><br><span class="line">mkdir -p &#x2F;mnt&#x2F;pmem0</span><br><span class="line">mkdir -p &#x2F;mnt&#x2F;pmem1</span><br><span class="line">mount -o dax &#x2F;dev&#x2F;pmem0 &#x2F;mnt&#x2F;pmem0</span><br><span class="line">mount -o dax &#x2F;dev&#x2F;pmem1 &#x2F;mnt&#x2F;pmem1</span><br></pre></td></tr></table></figure>

<p>   In this case file systems are generated for 2 NUMA nodes, which can be checked by “numactl –hardware”. For a different number of NUMA nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to NUMA nodes.</p>
<p>   For more information you can refer to <a target="_blank" rel="noopener" href="https://software.intel.com/content/www/us/en/develop/articles/quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux.html">Quick Start Guide: Provision Intel® Optane™ DC Persistent Memory</a></p>
<h4 id="Configuration-for-NUMA"><a href="#Configuration-for-NUMA" class="headerlink" title="Configuration for NUMA"></a>Configuration for NUMA</h4><ol>
<li><p>Install <code>numactl</code> to bind the executor to the PMem device on the same NUMA node. </p>
<p><code>yum install numactl -y </code></p>
</li>
<li><p>We strongly recommend you use NUMA-patched Spark to achieve better performance gain for the following 3 cache strategies. Besides, currently using Community Spark occasionally has the problem of two executors being bound to the same PMem path. </p>
<p>Build Spark from source to enable NUMA-binding support, refer to <a href="./Developer-Guide.md#Enabling-NUMA-binding-for-PMem-in-Spark">Enabling-NUMA-binding-for-PMem-in-Spark</a>. </p>
</li>
</ol>
<h4 id="Configuration-for-PMem"><a href="#Configuration-for-PMem" class="headerlink" title="Configuration for PMem"></a>Configuration for PMem</h4><p>Create <code>persistent-memory.xml</code> under <code>$SPARK_HOME/conf</code> if it doesn’t exist. Use the following template and change the <code>initialPath</code> to your mounted paths for PMem devices. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;persistentMemoryPool&gt;</span><br><span class="line">  &lt;!--The numa id--&gt;</span><br><span class="line">  &lt;numanode id&#x3D;&quot;0&quot;&gt;</span><br><span class="line">    &lt;!--The initial path for Intel Optane DC persistent memory--&gt;</span><br><span class="line">    &lt;initialPath&gt;&#x2F;mnt&#x2F;pmem0&lt;&#x2F;initialPath&gt;</span><br><span class="line">  &lt;&#x2F;numanode&gt;</span><br><span class="line">  &lt;numanode id&#x3D;&quot;1&quot;&gt;</span><br><span class="line">    &lt;initialPath&gt;&#x2F;mnt&#x2F;pmem1&lt;&#x2F;initialPath&gt;</span><br><span class="line">  &lt;&#x2F;numanode&gt;</span><br><span class="line">&lt;&#x2F;persistentMemoryPool&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Guava-cache"><a href="#Guava-cache" class="headerlink" title="Guava cache"></a>Guava cache</h3><p>Guava cache is based on memkind library, built on top of jemalloc and provides memory characteristics. To use it in your workload, follow <a href="#prerequisites">prerequisites</a> to set up PMem hardware correctly, also make sure memkind library installed. Then follow configurations below.</p>
<p><strong>NOTE</strong>: <code>spark.executor.sql.oap.cache.persistent.memory.reserved.size</code>: When we use PMem as memory through memkind library, some portion of the space needs to be reserved for memory management overhead, such as memory segmentation. We suggest reserving 20% - 25% of the available PMem capacity to avoid memory allocation failure. But even with an allocation failure, OAP will continue the operation to read data from original input data and will not cache the data block.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># enable numa</span><br><span class="line">spark.yarn.numa.enabled                                        true</span><br><span class="line">spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                   1</span><br><span class="line"># for Parquet file format, enable binary cache</span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled                     true</span><br><span class="line"># for ORC file format, enable binary cache</span><br><span class="line">spark.sql.oap.orc.binary.cache.enabled                         true</span><br><span class="line"></span><br><span class="line">spark.sql.oap.cache.memory.manager                             pm </span><br><span class="line">spark.oap.cache.strategy                                       guava</span><br><span class="line"># PMem capacity per executor, according to your cluster</span><br><span class="line">spark.executor.sql.oap.cache.persistent.memory.initial.size    256g</span><br><span class="line"># Reserved space per executor, according to your cluster</span><br><span class="line">spark.executor.sql.oap.cache.persistent.memory.reserved.size   50g</span><br><span class="line"># enable SQL Index and Data Source Cache jar in Spark</span><br><span class="line">spark.sql.extensions                                           org.apache.spark.sql.OapExtensions</span><br><span class="line"># absolute path of the jar on your working node, when in Yarn client mode</span><br><span class="line">spark.files                       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar,$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># relative path to spark.files, just specify jar name in current dir, when in Yarn client mode</span><br><span class="line">spark.executor.extraClassPath     .&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:.&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># absolute path of the jar on your working node,when in Yarn client mode</span><br><span class="line">spark.driver.extraClassPath       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Memkind library also support DAX KMEM mode. Refer to <a target="_blank" rel="noopener" href="https://github.com/memkind/memkind#kernel">Kernel</a>, this chapter will guide how to configure PMem as system RAM. Or <a target="_blank" rel="noopener" href="https://pmem.io/2020/01/20/memkind-dax-kmem.html">Memkind support for KMEM DAX option</a> for more details.</p>
<p>Please note that DAX KMEM mode need kernel version 5.x and memkind version 1.10 or above. If you choose KMEM mode, change memory manager from <code>pm</code> to <code>kmem</code> as below.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.oap.cache.memory.manager           kmem</span><br></pre></td></tr></table></figure>

<h3 id="Noevict-cache"><a href="#Noevict-cache" class="headerlink" title="Noevict cache"></a>Noevict cache</h3><p>The noevict cache strategy is also supported in OAP based on the memkind library for PMem.</p>
<p>To use it in your workload, follow <a href="#prerequisites">prerequisites</a> to set up PMem hardware correctly, also make sure memkind library installed. Then follow the configuration below.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># enable numa</span><br><span class="line">spark.yarn.numa.enabled                                      true</span><br><span class="line">spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                 1</span><br><span class="line"># for Parquet file format, enable binary cache</span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled                   true </span><br><span class="line"># for ORC file format, enable binary cache</span><br><span class="line">spark.sql.oap.orc.binary.cache.enabled                       true</span><br><span class="line">spark.oap.cache.strategy                                     noevict </span><br><span class="line">spark.executor.sql.oap.cache.persistent.memory.initial.size  256g </span><br><span class="line"></span><br><span class="line"># Enable OAP extension in Spark</span><br><span class="line">spark.sql.extensions              org.apache.spark.sql.OapExtensions</span><br><span class="line"></span><br><span class="line"># absolute path of the jar on your working node, when in Yarn client mode</span><br><span class="line">spark.files                       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar,$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># relative path to spark.files, just specify jar name in current dir, when in Yarn client mode</span><br><span class="line">spark.executor.extraClassPath     .&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:.&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># absolute path of the jar on your working node,when in Yarn client mode</span><br><span class="line">spark.driver.extraClassPath       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Vmemcache"><a href="#Vmemcache" class="headerlink" title="Vmemcache"></a>Vmemcache</h3><ul>
<li><p>Make sure <a target="_blank" rel="noopener" href="https://github.com/pmem/vmemcache">Vmemcache</a> library has been installed on every cluster worker node if vmemcache strategy is chosen for PMem cache. If you have finished <a href="../../docs/OAP-Installation-Guide.md">OAP-Installation-Guide</a>, vmemcache library will be automatically installed by Conda.</p>
<p>Or you can follow the <a href="./Developer-Guide.md#build-and-install-vmemcache">build/install</a> steps and make sure <code>libvmemcache.so</code> exist in <code>/lib64</code> directory on each worker node.</p>
</li>
<li><p>To use it in your workload, follow <a href="#prerequisites">prerequisites</a> to set up PMem hardware correctly.</p>
</li>
</ul>
<h4 id="Configure-to-enable-PMem-cache"><a href="#Configure-to-enable-PMem-cache" class="headerlink" title="Configure to enable PMem cache"></a>Configure to enable PMem cache</h4><p>Make the following configuration changes in <code>$SPARK_HOME/conf/spark-defaults.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 2x number of your worker nodes</span><br><span class="line">spark.executor.instances          6</span><br><span class="line"># enable numa</span><br><span class="line">spark.yarn.numa.enabled           true</span><br><span class="line"># Enable OAP extension in Spark</span><br><span class="line">spark.sql.extensions              org.apache.spark.sql.OapExtensions</span><br><span class="line"></span><br><span class="line"># absolute path of the jar on your working node, when in Yarn client mode</span><br><span class="line">spark.files                       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar,$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># relative path to spark.files, just specify jar name in current dir, when in Yarn client mode</span><br><span class="line">spark.executor.extraClassPath     .&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:.&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># absolute path of the jar on your working node,when in Yarn client mode</span><br><span class="line">spark.driver.extraClassPath       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"></span><br><span class="line"># for parquet file format, enable binary cache</span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled                   true</span><br><span class="line"># for ORC file format, enable binary cache</span><br><span class="line">spark.sql.oap.orc.binary.cache.enabled                       true</span><br><span class="line"># enable vmemcache strategy </span><br><span class="line">spark.oap.cache.strategy                                     vmem </span><br><span class="line">spark.executor.sql.oap.cache.persistent.memory.initial.size  256g </span><br><span class="line"># according to your cluster</span><br><span class="line">spark.executor.sql.oap.cache.guardian.memory.size            10g</span><br></pre></td></tr></table></figure>
<p>The <code>vmem</code> cache strategy is based on libvmemcache (buffer based LRU cache), which provides a key-value store API. Follow these steps to enable vmemcache support in Data Source Cache.</p>
<ul>
<li><code>spark.executor.instances</code>: We suggest setting the value to 2X the number of worker nodes when NUMA binding is enabled. Each worker node runs two executors, each executor is bound to one of the two sockets, and accesses the corresponding PMem device on that socket.</li>
<li><code>spark.executor.sql.oap.cache.persistent.memory.initial.size</code>: It is configured to the available PMem capacity to be used as data cache per exectutor.</li>
</ul>
<p><strong>NOTE</strong>: If “PendingFiber Size” (on spark web-UI OAP page) is large, or some tasks fail with “cache guardian use too much memory” error, set <code>spark.executor.sql.oap.cache.guardian.memory.size </code> to a larger number as the default size is 10GB. The user could also increase <code>spark.sql.oap.cache.guardian.free.thread.nums</code> or decrease <code>spark.sql.oap.cache.dispose.timeout.ms</code> to free memory more quickly.</p>
<h4 id="Verify-PMem-cache-functionality"><a href="#Verify-PMem-cache-functionality" class="headerlink" title="Verify PMem cache functionality"></a>Verify PMem cache functionality</h4><ul>
<li><p>After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the <a href="./User-Guide.md#use-dram-cache">Use DRAM Cache</a> guide to verify that cache is working correctly.</p>
</li>
<li><p>Verify NUMA binding status by confirming keywords like <code>numactl --cpubind=1 --membind=1</code> contained in executor launch command.</p>
</li>
<li><p>Check PMem cache size by checking disk space with <code>df -h</code>.For <code>vmemcache</code> strategy, disk usage will reach the initial cache size once the PMem cache is initialized and will not change during workload execution. For <code>Guava/Noevict</code> strategies, the command will show disk space usage increases along with workload execution. </p>
</li>
</ul>
<h2 id="Index-and-Data-Cache-Separation"><a href="#Index-and-Data-Cache-Separation" class="headerlink" title="Index and Data Cache Separation"></a>Index and Data Cache Separation</h2><p>SQL Index and Data Source Cache now supports different cache strategies for DRAM and PMem. To optimize the cache media utilization, you can enable cache separation of data and index with same or different cache media. When Sharing same media, data cache and index cache will use different fiber cache ratio.</p>
<p>Here we list 4 different kinds of configuration for index/cache separation, if you choose one of them, please add corresponding configuration to <code>spark-defaults.conf</code>.</p>
<ol>
<li>DRAM as cache media, <code>guava</code> strategy as index &amp; data cache backend. </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.oap.index.data.cache.separation.enabled       true</span><br><span class="line">spark.oap.cache.strategy                                mix</span><br><span class="line">spark.sql.oap.cache.memory.manager                      offheap</span><br></pre></td></tr></table></figure>
<p>The rest configuration you can refer to  <a href="./User-Guide.md#use-dram-cache">Use DRAM Cache</a> </p>
<ol start="2">
<li>PMem as cache media, <code>external</code> strategy as index &amp; data cache backend. </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.oap.index.data.cache.separation.enabled       true</span><br><span class="line">spark.oap.cache.strategy                                mix</span><br><span class="line">spark.sql.oap.cache.memory.manager                      tmp</span><br><span class="line">spark.sql.oap.mix.data.cache.backend                    external</span><br><span class="line">spark.sql.oap.mix.index.cache.backend                   external</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>The rest configurations can refer to the configurations of <a href="./User-Guide.md#use-pmem-cache">PMem Cache</a> and  <a href="./User-Guide.md#Configuration-for-enabling-PMem-cache">External cache</a></p>
<ol start="3">
<li>DRAM(<code>offheap</code>)/<code>guava</code> as <code>index</code> cache media and backend, PMem(<code>tmp</code>)/<code>external</code> as <code>data</code> cache media and backend. </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.oap.index.data.cache.separation.enabled            true</span><br><span class="line">spark.oap.cache.strategy                                     mix</span><br><span class="line">spark.sql.oap.cache.memory.manager                           mix </span><br><span class="line">spark.sql.oap.mix.data.cache.backend                         external</span><br><span class="line"></span><br><span class="line"># 2x number of your worker nodes</span><br><span class="line">spark.executor.instances                                     6</span><br><span class="line"># enable numa</span><br><span class="line">spark.yarn.numa.enabled                                      true</span><br><span class="line">spark.memory.offHeap.enabled                                 false</span><br><span class="line"></span><br><span class="line">spark.sql.oap.dcpmm.free.wait.threshold                      50000000000</span><br><span class="line"># according to your executor core number</span><br><span class="line">spark.executor.sql.oap.cache.external.client.pool.size       10</span><br><span class="line"></span><br><span class="line"># equal to the size of executor.memoryOverhead</span><br><span class="line">spark.executor.sql.oap.cache.offheap.memory.size             50g</span><br><span class="line"># according to the resource of cluster</span><br><span class="line">spark.executor.memoryOverhead                                50g</span><br><span class="line"></span><br><span class="line"># for ORC file format</span><br><span class="line">spark.sql.oap.orc.binary.cache.enabled                       true</span><br><span class="line"># for Parquet file format</span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled                   true</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>DRAM(<code>offheap</code>)/<code>guava</code> as <code>index</code> cache media and backend, PMem(<code>pm</code>)/<code>guava</code> as <code>data</code> cache media and backend. </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.oap.index.data.cache.separation.enabled            true</span><br><span class="line">spark.oap.cache.strategy                                     mix</span><br><span class="line">spark.sql.oap.cache.memory.manager                           mix </span><br><span class="line"></span><br><span class="line"># 2x number of your worker nodes</span><br><span class="line">spark.executor.instances                                     6</span><br><span class="line"># enable numa</span><br><span class="line">spark.yarn.numa.enabled                                      true</span><br><span class="line">spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                 1</span><br><span class="line">spark.memory.offHeap.enabled                                 false</span><br><span class="line"># PMem capacity per executor</span><br><span class="line">spark.executor.sql.oap.cache.persistent.memory.initial.size  256g</span><br><span class="line"># Reserved space per executor</span><br><span class="line">spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g</span><br><span class="line"></span><br><span class="line"># equal to the size of executor.memoryOverhead</span><br><span class="line">spark.executor.sql.oap.cache.offheap.memory.size             50g</span><br><span class="line"># according to the resource of cluster</span><br><span class="line">spark.executor.memoryOverhead                                50g</span><br><span class="line"># for ORC file format</span><br><span class="line">spark.sql.oap.orc.binary.cache.enabled                       true</span><br><span class="line"># for Parquet file format</span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled                   true</span><br></pre></td></tr></table></figure>

<h2 id="Cache-Hot-Tables"><a href="#Cache-Hot-Tables" class="headerlink" title="Cache Hot Tables"></a>Cache Hot Tables</h2><p>Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.</p>
<p>To enable caching specific hot tables, you can add the configuration below to <code>spark-defaults.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># enable table lists fiberCache</span><br><span class="line">spark.sql.oap.cache.table.list.enabled          true</span><br><span class="line"># Table lists using fiberCache actively</span><br><span class="line">spark.sql.oap.cache.table.list                  &lt;databasename&gt;.&lt;tablename1&gt;;&lt;databasename&gt;.&lt;tablename2&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Column-Vector-Cache"><a href="#Column-Vector-Cache" class="headerlink" title="Column Vector Cache"></a>Column Vector Cache</h2><p>This document above use <strong>binary</strong> cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time. </p>
<p>To enable ColumnVector data cache for Parquet file format, you should add the configuration below to <code>spark-defaults.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># for parquet file format, disable binary cache</span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled             false</span><br><span class="line"># for parquet file format, enable ColumnVector cache</span><br><span class="line">spark.sql.oap.parquet.data.cache.enabled               true</span><br></pre></td></tr></table></figure>

<h2 id="Large-Scale-and-Heterogeneous-Cluster-Support"><a href="#Large-Scale-and-Heterogeneous-Cluster-Support" class="headerlink" title="Large Scale and Heterogeneous Cluster Support"></a>Large Scale and Heterogeneous Cluster Support</h2><p><strong><em>NOTE:</em></strong> Only works with <code>external cache</code></p>
<p>OAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a <strong><em>large scale cluster</em></strong>, and how to schedule tasks in a <strong><em>heterogeneous cluster</em></strong> (some nodes with PMem, some without) could also be challenging.</p>
<p>We introduce an external DB to store cache locality info. If there’s no cache available, Spark will fall back to schedule respecting HDFS locality.<br>Currently we support <a target="_blank" rel="noopener" href="https://redis.io/">Redis</a> as external DB service. Please <a target="_blank" rel="noopener" href="https://redis.io/download">download and launch a redis-server</a> before running Spark with OAP.</p>
<p>Please add the following configurations to <code>spark-defaults.conf</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.oap.external.cache.metaDB.enabled            true</span><br><span class="line"># Redis-server address</span><br><span class="line">spark.sql.oap.external.cache.metaDB.address            10.1.2.12</span><br><span class="line">spark.sql.oap.external.cache.metaDB.impl               org.apache.spark.sql.execution.datasources.RedisClient</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://hongw2019.github.io/OAP-1.0.0/2020/12/17/Advanced-Configuration/" data-id="ckisy2kkx0001nfpvaj9s32qd" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-User-Guide" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/OAP-1.0.0/2020/12/17/User-Guide/" class="article-date">
  <time class="dt-published" datetime="2020-12-17T14:29:09.052Z" itemprop="datePublished">2020-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="User-Guide"><a href="#User-Guide" class="headerlink" title="User Guide"></a>User Guide</h1><ul>
<li><a href="#Prerequisites">Prerequisites</a></li>
<li><a href="#Getting-Started">Getting Started</a></li>
<li><a href="#Configuration-for-YARN-Cluster-Mode">Configuration for YARN Cluster Mode</a></li>
<li><a href="#Configuration-for-Spark-Standalone-Mode">Configuration for Spark Standalone Mode</a></li>
<li><a href="#Working-with-SQL-Index">Working with SQL Index</a></li>
<li><a href="#Working-with-SQL-Data-Source-Cache">Working with SQL Data Source Cache</a></li>
<li><a href="#Run-TPC-DS-Benchmark">Run TPC-DS Benchmark</a></li>
<li><a href="#Advanced-Configuration">Advanced Configuration</a></li>
</ul>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support.</p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h3 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h3><p>We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow <a href="../../../docs/OAP-Installation-Guide.md">OAP-Installation-Guide</a> and you can find compiled OAP jars under<br> <code>$HOME/miniconda2/envs/oapenv/oap_jars</code> once finished the installation.</p>
<p>If you’d like to build from source code, please refer to <a href="Developer-Guide.md">Developer Guide</a> for the detailed steps.</p>
<h3 id="Spark-Configurations"><a href="#Spark-Configurations" class="headerlink" title="Spark Configurations"></a>Spark Configurations</h3><p>Users usually test and run Spark SQL or Scala scripts in Spark Shell,  which launches Spark applications on YRAN with <strong><em>client</em></strong> mode. In this section, we will start with Spark Shell then introduce other use scenarios. </p>
<p>Before you run <code>$SPARK_HOME/bin/spark-shell </code>, you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file <code>$SPARK_HOME/conf/spark-defaults.conf</code> on your working node.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.extensions              org.apache.spark.sql.OapExtensions</span><br><span class="line"><span class="comment"># absolute path of the jar on your working node</span></span><br><span class="line">spark.files                       <span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar,<span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"><span class="comment"># relative path to spark.files, just specify jar name in current dir</span></span><br><span class="line">spark.executor.extraClassPath     ./oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:./oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"><span class="comment"># absolute path of the jar on your working node</span></span><br><span class="line">spark.driver.extraClassPath       <span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:<span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br></pre></td></tr></table></figure>
<h3 id="Verify-Integration"><a href="#Verify-Integration" class="headerlink" title="Verify Integration"></a>Verify Integration</h3><p>After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell.</p>
<ol>
<li><p>Create a test data path on your HDFS. <code>hdfs:///user/oap/</code> for example.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /user/oap/</span><br></pre></td></tr></table></figure></li>
<li><p>Launch Spark Shell using the following command on your working node.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-shell</span><br></pre></td></tr></table></figure>
</li>
<li><p>Execute the following commands in Spark Shell to test OAP integration. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(s&quot;&quot;&quot;CREATE TABLE oap_test (a INT, b STRING)</span><br><span class="line">       USING parquet</span><br><span class="line">       OPTIONS (path &#39;hdfs:&#x2F;&#x2F;&#x2F;user&#x2F;oap&#x2F;&#39;)&quot;&quot;&quot;.stripMargin)</span><br><span class="line">&gt; val data &#x3D; (1 to 30000).map &#123; i &#x3D;&gt; (i, s&quot;this is test $i&quot;) &#125;.toDF().createOrReplaceTempView(&quot;t&quot;)</span><br><span class="line">&gt; spark.sql(&quot;insert overwrite table oap_test select * from t&quot;)</span><br><span class="line">&gt; spark.sql(&quot;create oindex index1 on oap_test (a)&quot;)</span><br><span class="line">&gt; spark.sql(&quot;show oindex from oap_test&quot;).show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>This test creates an index for a table and then shows it. If there are no errors, the OAP <code>.jar</code> is working with the configuration. The picture below is an example of a successfully run.</p>
<p><img src="./image/spark_shell_oap.png" alt="Spark_shell_running_results"></p>
<h2 id="Configuration-for-YARN-Cluster-Mode"><a href="#Configuration-for-YARN-Cluster-Mode" class="headerlink" title="Configuration for YARN Cluster Mode"></a>Configuration for YARN Cluster Mode</h2><p>Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in <strong><em>client</em></strong> mode. While Spark Submit tool can run Spark application in <strong><em>client</em></strong> or <strong><em>cluster</em></strong> mode, which is decided by <code>--deploy-mode</code> parameter. <a href="#Getting-Started">Getting Started</a> session has shown the configurations needed for <strong><em>client</em></strong> mode. If you are running Spark Submit tool in <strong><em>cluster</em></strong> mode, you need to follow the below configuration steps instead.</p>
<p>Add the following OAP configuration settings to <code>$SPARK_HOME/conf/spark-defaults.conf</code> on your working node before running <code>spark-submit</code> in <strong><em>cluster</em></strong> mode.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.extensions              org.apache.spark.sql.OapExtensions</span><br><span class="line"><span class="comment"># absolute path on your working node</span></span><br><span class="line">spark.files                       <span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar,<span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"><span class="comment"># relative path to spark.files, just specify jar name in current dir   </span></span><br><span class="line">spark.executor.extraClassPath     ./oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:./oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"><span class="comment"># relative path to spark.files, just specify jar name in current dir</span></span><br><span class="line">spark.driver.extraClassPath       ./oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:./oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br></pre></td></tr></table></figure>

<h2 id="Configuration-for-Spark-Standalone-Mode"><a href="#Configuration-for-Spark-Standalone-Mode" class="headerlink" title="Configuration for Spark Standalone Mode"></a>Configuration for Spark Standalone Mode</h2><p>In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode:</p>
<ol>
<li>Make sure the OAP <code>.jar</code> at the same path of <strong>all</strong> the worker nodes.</li>
<li>Add the following configuration settings to <code>$SPARK_HOME/conf/spark-defaults.conf</code> on the working node.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.extensions               org.apache.spark.sql.OapExtensions</span><br><span class="line"><span class="comment"># absolute path on worker nodes</span></span><br><span class="line">spark.executor.extraClassPath      <span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:<span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"><span class="comment"># absolute path on worker nodes</span></span><br><span class="line">spark.driver.extraClassPath        <span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:<span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Working-with-SQL-Index"><a href="#Working-with-SQL-Index" class="headerlink" title="Working with SQL Index"></a>Working with SQL Index</h2><p>After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include <code>index create</code>, <code>drop</code>, <code>refresh</code>, and <code>show</code>. Test these functions using the following examples in Spark Shell.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(s<span class="string">&quot;&quot;</span><span class="string">&quot;CREATE TABLE oap_test (a INT, b STRING)</span></span><br><span class="line"><span class="string">       USING parquet</span></span><br><span class="line"><span class="string">       OPTIONS (path &#x27;hdfs:///user/oap/&#x27;)&quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line">&gt; val data = (1 to 30000).map &#123; i =&gt; (i, s<span class="string">&quot;this is test <span class="variable">$i</span>&quot;</span>) &#125;.toDF().createOrReplaceTempView(<span class="string">&quot;t&quot;</span>)</span><br><span class="line">&gt; spark.sql(<span class="string">&quot;insert overwrite table oap_test select * from t&quot;</span>)       </span><br></pre></td></tr></table></figure>

<h3 id="Index-Creation"><a href="#Index-Creation" class="headerlink" title="Index Creation"></a>Index Creation</h3><p>Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP]</span><br></pre></td></tr></table></figure>
<p>The following example creates a B+ Tree index on column “a” of the <code>oap_test</code> table.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(<span class="string">&quot;create oindex index1 on oap_test (a)&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>Use SHOW OINDEX command to show all the created indexes on a specified table.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(<span class="string">&quot;show oindex from oap_test&quot;</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="Use-Index"><a href="#Use-Index" class="headerlink" title="Use Index"></a>Use Index</h3><p>Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column “a”.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(<span class="string">&quot;SELECT * FROM oap_test WHERE a = 1&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="Drop-index"><a href="#Drop-index" class="headerlink" title="Drop index"></a>Drop index</h3><p>Use DROP OINDEX command to drop a named index.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(<span class="string">&quot;drop oindex index1 on oap_test&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Working-with-SQL-Data-Source-Cache"><a href="#Working-with-SQL-Data-Source-Cache" class="headerlink" title="Working with SQL Data Source Cache"></a>Working with SQL Data Source Cache</h2><p>Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware.</p>
<h3 id="Use-DRAM-Cache"><a href="#Use-DRAM-Cache" class="headerlink" title="Use DRAM Cache"></a>Use DRAM Cache</h3><ol>
<li><p>Make the following configuration changes in Spark configuration file <code>$SPARK_HOME/conf/spark-defaults.conf</code>. </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark.memory.offHeap.enabled                      <span class="literal">false</span></span><br><span class="line">spark.oap.cache.strategy                          guava</span><br><span class="line">spark.sql.oap.cache.memory.manager                offheap</span><br><span class="line"><span class="comment"># according to the resource of cluster</span></span><br><span class="line">spark.executor.memoryOverhead                     50g</span><br><span class="line"><span class="comment"># equal to the size of executor.memoryOverhead</span></span><br><span class="line">spark.executor.sql.oap.cache.offheap.memory.size  50g</span><br><span class="line"><span class="comment"># for parquet fileformat, enable binary cache</span></span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled        <span class="literal">true</span></span><br><span class="line"><span class="comment"># for orc fileformat, enable binary cache</span></span><br><span class="line">spark.sql.oap.orc.binary.cache.enabled            <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p><strong><em>NOTE</em></strong>: Change <code>spark.executor.sql.oap.cache.offheap.memory.size</code> based on the availability of DRAM capacity to cache data, and its size is equal to <code>spark.executor.memoryOverhead</code></p>
</li>
<li><p>Launch Spark <strong><em>ThriftServer</em></strong></p>
<p>Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. </p>
<p>The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the <a href="#Working-with-SQL-Index">Working with SQL Index</a> section, which creates the <code>oap_test</code> table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL’s through Beeline for creating your tables.</p>
<p>When you run <code>spark-shell</code> to create the <code>oap_test</code> table, <code>metastore_db</code> will be created in the directory where you ran ‘$SPARK_HOME/bin/spark-shell’. <strong><em>Go to that directory</em></strong> and execute the following command to launch Thrift JDBC server and run queries.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$SPARK_HOME</span>/sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">. <span class="variable">$SPARK_HOME</span>/bin/beeline -u jdbc:hive2://&lt;mythriftserver&gt;:10000       </span><br></pre></td></tr></table></figure>

<p>After the connection is established, execute the following commands to check the metastore is initialized correctly.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; SHOW databases;</span><br><span class="line">&gt; USE default;</span><br><span class="line">&gt; SHOW tables;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Run queries on the table that will use the cache automatically. For example,</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; SELECT * FROM oap_test WHERE a = 1;</span><br><span class="line">&gt; SELECT * FROM oap_test WHERE a = 2;</span><br><span class="line">&gt; SELECT * FROM oap_test WHERE a = 3;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example.</p>
<p><img src="./image/webUI.png" alt="webUI"></p>
</li>
</ol>
<h3 id="Use-PMem-Cache"><a href="#Use-PMem-Cache" class="headerlink" title="Use PMem Cache"></a>Use PMem Cache</h3><h4 id="Prerequisites-1"><a href="#Prerequisites-1" class="headerlink" title="Prerequisites"></a>Prerequisites</h4><p>The following steps are required to configure OAP to use PMem cache with <code>external</code> cache strategy.</p>
<ul>
<li><p>PMem hardware is successfully deployed on each node in cluster.</p>
</li>
<li><p>Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Socket Configuration -&gt; Memory Configuration -&gt; NGN Configuration -&gt; Snoopy mode <span class="keyword">for</span> AD : Enabled</span><br><span class="line">Socket Configuration -&gt; Intel UPI General Configuration -&gt; Stale AtoS :  Disabled</span><br></pre></td></tr></table></figure>

<ul>
<li>It’s strongly advised to use <a target="_blank" rel="noopener" href="https://pmem.io/2018/05/15/using_persistent_memory_devices_with_the_linux_device_mapper.html">Linux device mapper</a> to interleave PMem across sockets and get maximum size for Plasma.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// use ipmctl <span class="built_in">command</span> to show topology and dimm info of PMem</span><br><span class="line">ipmctl show -topology</span><br><span class="line">ipmctl show -dimm</span><br><span class="line">// provision PMem <span class="keyword">in</span> app direct mode</span><br><span class="line">ipmctl create -goal PersistentMemoryType=AppDirect</span><br><span class="line">// reboot system to make configuration take affect</span><br><span class="line">reboot</span><br><span class="line">// check capacity provisioned <span class="keyword">for</span> app direct mode(AppDirectCapacity)</span><br><span class="line">ipmctl show -memoryresources</span><br><span class="line">// show the PMem region information</span><br><span class="line">ipmctl show -region</span><br><span class="line">// create namespace based on the region, multi namespaces can be created on a single region</span><br><span class="line">ndctl create-namespace -m fsdax -r region0</span><br><span class="line">ndctl create-namespace -m fsdax -r region1</span><br><span class="line">// show the created namespaces</span><br><span class="line">fdisk -l</span><br><span class="line">// create and mount file system</span><br><span class="line">sudo dmsetup create striped-pmem</span><br><span class="line">mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem</span><br><span class="line">mkdir -p /mnt/pmem</span><br><span class="line">mount -o dax /dev/mapper/striped-pmem /mnt/pmem</span><br></pre></td></tr></table></figure>

<p>   For more information you can refer to <a target="_blank" rel="noopener" href="https://software.intel.com/content/www/us/en/develop/articles/quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux.html">Quick Start Guide: Provision Intel® Optane™ DC Persistent Memory</a></p>
<ul>
<li>SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries.  <a target="_blank" rel="noopener" href="http://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/">Plasma</a> is a high-performance shared-memory object store and a component of <a target="_blank" rel="noopener" href="https://github.com/apache/arrow">Apache Arrow</a>. We have modified Plasma to support PMem, and make it open source on <a target="_blank" rel="noopener" href="https://github.com/Intel-bigdata/arrow/tree/branch-0.17.0-oap-1.0">Intel-bigdata Arrow</a> repo. If you have finished <a href="../../../docs/OAP-Installation-Guide.md">OAP Installation Guide</a>, Plasma will be automatically installed and then you just need copy <code>arrow-plasma-0.17.0.jar</code> to <code>$SPARK_HOME/jars</code>. For manual building and installation steps you can refer to <a href="./Developer-Guide.md#Plasma-installation">Plasma installation</a>.</li>
</ul>
<ul>
<li>Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload.</li>
</ul>
<h4 id="Configuration-for-NUMA"><a href="#Configuration-for-NUMA" class="headerlink" title="Configuration for NUMA"></a>Configuration for NUMA</h4><p>Install <code>numactl</code> to bind the executor to the PMem device on the same NUMA node. </p>
<p>   <code>yum install numactl -y </code></p>
<h4 id="Configuration-for-enabling-PMem-cache"><a href="#Configuration-for-enabling-PMem-cache" class="headerlink" title="Configuration for enabling PMem cache"></a>Configuration for enabling PMem cache</h4><p>Add the following configuration to <code>$SPARK_HOME/conf/spark-defaults.conf</code>.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2x number of your worker nodes</span></span><br><span class="line">spark.executor.instances          6</span><br><span class="line"><span class="comment"># enable numa</span></span><br><span class="line">spark.yarn.numa.enabled           <span class="literal">true</span></span><br><span class="line"><span class="comment"># enable SQL Index and Data Source Cache extension in Spark</span></span><br><span class="line">spark.sql.extensions              org.apache.spark.sql.OapExtensions</span><br><span class="line"></span><br><span class="line"><span class="comment"># absolute path of the jar on your working node, when in Yarn client mode</span></span><br><span class="line">spark.files                       <span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar,<span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"><span class="comment"># relative path to spark.files, just specify jar name in current dir, when in Yarn client mode</span></span><br><span class="line">spark.executor.extraClassPath     ./oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:./oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"><span class="comment"># absolute path of the jar on your working node,when in Yarn client mode</span></span><br><span class="line">spark.driver.extraClassPath       <span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:<span class="variable">$HOME</span>/miniconda2/envs/oapenv/oap_jars/oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"></span><br><span class="line"><span class="comment"># for parquet file format, enable binary cache</span></span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled                   <span class="literal">true</span></span><br><span class="line"><span class="comment"># for ORC file format, enable binary cache</span></span><br><span class="line">spark.sql.oap.orc.binary.cache.enabled                       <span class="literal">true</span></span><br><span class="line"><span class="comment"># enable external cache strategy </span></span><br><span class="line">spark.oap.cache.strategy                                     external </span><br><span class="line">spark.sql.oap.dcpmm.free.wait.threshold                      50000000000</span><br><span class="line"><span class="comment"># according to your executor core number</span></span><br><span class="line">spark.executor.sql.oap.cache.external.client.pool.size       10</span><br></pre></td></tr></table></figure>
<p>Start Plasma service manually</p>
<p>Plasma config parameters:  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-m  how much Bytes share memory Plasma will use</span><br><span class="line">-s  Unix Domain sockcet path</span><br><span class="line">-d  PMem directory</span><br></pre></td></tr></table></figure>

<p>Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find <code>plasma-store-server</code> in the path <strong>$HOME/miniconda2/envs/oapenv/bin/</strong>.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem  </span><br></pre></td></tr></table></figure>

<p>Remember to kill <code>plasma-store-server</code> process if you no longer need cache, and you should delete <code>/tmp/plasmaStore</code> which is a Unix domain socket.  </p>
<ul>
<li>Use Yarn to start Plamsa service<br>When using Yarn(Hadoop version &gt;= 3.1) to start Plasma service, you should provide a json file as below.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;name&quot;</span>: <span class="string">&quot;plasma-store-service&quot;</span>,</span><br><span class="line">  <span class="string">&quot;version&quot;</span>: 1,</span><br><span class="line">  <span class="string">&quot;components&quot;</span> :</span><br><span class="line">  [</span><br><span class="line">   &#123;</span><br><span class="line">     <span class="string">&quot;name&quot;</span>: <span class="string">&quot;plasma-store-service&quot;</span>,</span><br><span class="line">     <span class="string">&quot;number_of_containers&quot;</span>: 3,</span><br><span class="line">     <span class="string">&quot;launch_command&quot;</span>: <span class="string">&quot;plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem&quot;</span>,</span><br><span class="line">     <span class="string">&quot;resource&quot;</span>: &#123;</span><br><span class="line">       <span class="string">&quot;cpus&quot;</span>: 1,</span><br><span class="line">       <span class="string">&quot;memory&quot;</span>: 512</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>Run command  <code>yarn app -launch plasma-store-service /tmp/plasmaLaunch.json</code> to start Plasma server.<br>Run <code>yarn app -stop plasma-store-service</code> to stop it.<br>Run <code>yarn app -destroy plasma-store-service</code>to destroy it.</p>
<h3 id="Verify-PMem-cache-functionality"><a href="#Verify-PMem-cache-functionality" class="headerlink" title="Verify PMem cache functionality"></a>Verify PMem cache functionality</h3><ul>
<li><p>After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the <a href="#use-dram-cache">Use DRAM Cache</a> guide to verify that cache is working correctly.</p>
</li>
<li><p>Check PMem cache size by checking disk space with <code>df -h</code>.</p>
</li>
</ul>
<h2 id="Run-TPC-DS-Benchmark"><a href="#Run-TPC-DS-Benchmark" class="headerlink" title="Run TPC-DS Benchmark"></a>Run TPC-DS Benchmark</h2><p>This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation.</p>
<p>We created some tool scripts <a target="_blank" rel="noopener" href="https://github.com/Intel-bigdata/OAP/releases/download/v0.9.0-spark-3.0.0/oap-benchmark-tool.zip">oap-benchmark-tool.zip</a> to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly.</p>
<h3 id="Prerequisites-2"><a href="#Prerequisites-2" class="headerlink" title="Prerequisites"></a>Prerequisites</h3><ul>
<li>Python 2.7+ is required on the working node. </li>
</ul>
<h3 id="Prepare-the-Tool"><a href="#Prepare-the-Tool" class="headerlink" title="Prepare the Tool"></a>Prepare the Tool</h3><ol>
<li>Download <a target="_blank" rel="noopener" href="https://github.com/Intel-bigdata/OAP/releases/download/v0.9.0-spark-3.0.0/oap-benchmark-tool.zip">oap-benchmark-tool.zip</a> and unzip to a folder (for example, <code>oap-benchmark-tool</code> folder) on your working node. </li>
<li>Copy <code>oap-benchmark-tool/tools/tpcds-kits</code> to <strong><em>ALL</em></strong> worker nodes under the same folder (for example, <code>/home/oap/tpcds-kits</code>).</li>
</ol>
<h3 id="Generate-TPC-DS-Data"><a href="#Generate-TPC-DS-Data" class="headerlink" title="Generate TPC-DS Data"></a>Generate TPC-DS Data</h3><ol>
<li><p>Update the values for the following variables in <code>oap-benchmark-tool/scripts/tool.conf</code> based on your environment and needs.</p>
<ul>
<li>SPARK_HOME: Point to the Spark home directory of your Spark setup.</li>
<li>TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits</li>
<li>NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port.</li>
<li>THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server.</li>
<li>DATA_SCALE: The data scale to be generated in GB</li>
<li>DATA_FORMAT: The data file format. You can specify parquet or orc</li>
</ul>
<p>For example:</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/oap/spark-3.0.0</span><br><span class="line"><span class="built_in">export</span> TPCDS_KITS_DIR=/home/oap/tpcds-kits</span><br><span class="line"><span class="built_in">export</span> NAMENODE_ADDRESS=mynamenode:9000</span><br><span class="line"><span class="built_in">export</span> THRIFT_SERVER_ADDRESS=mythriftserver</span><br><span class="line"><span class="built_in">export</span> DATA_SCALE=1024</span><br><span class="line"><span class="built_in">export</span> DATA_FORMAT=parquet</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>Start data generation.</p>
<p>In the root directory of this tool (<code>oap-benchmark-tool</code>), run <code>scripts/run_gen_data.sh</code> to start the data generation process. </p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> oap-benchmark-tool</span><br><span class="line">sh ./scripts/run_gen_data.sh</span><br></pre></td></tr></table></figure>

<p>   Once finished, the <code>$scale</code> data will be generated in the HDFS folder <code>genData$scale</code>. And a database called <code>tpcds_$format$scale</code> will contain the TPC-DS tables.</p>
<h3 id="Start-Spark-Thrift-Server"><a href="#Start-Spark-Thrift-Server" class="headerlink" title="Start Spark Thrift Server"></a>Start Spark Thrift Server</h3><p>Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server.</p>
<h4 id="Use-PMem-as-Cache-Media"><a href="#Use-PMem-as-Cache-Media" class="headerlink" title="Use PMem as Cache Media"></a>Use PMem as Cache Media</h4><p>Update the configuration values in <code>scripts/spark_thrift_server_yarn_with_PMem.sh</code> to reflect your environment.<br>Normally, you need to update the following configuration values to cache to PMem.</p>
<ul>
<li>–num-executors</li>
<li>–driver-memory</li>
<li>–executor-memory</li>
<li>–executor-cores</li>
<li>–conf spark.oap.cache.strategy</li>
<li>–conf spark.sql.oap.dcpmm.free.wait.threshold</li>
<li>–conf spark.executor.sql.oap.cache.external.client.pool.size</li>
</ul>
<p>These settings will override the values specified in Spark configuration file ( <code>spark-defaults.conf</code>). After the configuration is done, you can execute the following command to start Thrift Server.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> oap-benchmark-tool</span><br><span class="line">sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start</span><br></pre></td></tr></table></figure>
<p>In this script, we use <code>external</code> as cache strategy for Parquet Binary data cache. </p>
<h4 id="Use-DRAM-as-Cache-Media"><a href="#Use-DRAM-as-Cache-Media" class="headerlink" title="Use DRAM as Cache Media"></a>Use DRAM as Cache Media</h4><p>Update the configuration values in <code>scripts/spark_thrift_server_yarn_with_DRAM.sh</code> to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM.</p>
<ul>
<li>–num-executors</li>
<li>–driver-memory</li>
<li>–executor-memory</li>
<li>–executor-cores</li>
<li>–conf spark.executor.sql.oap.cache.offheap.memory.size</li>
<li>–conf spark.executor.memoryOverhead</li>
</ul>
<p>These settings will override the values specified in Spark configuration file (<code>spark-defaults.conf</code>). After the configuration is done, you can execute the following command to start Thrift Server.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> oap-benchmark-tool</span><br><span class="line">sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh  start</span><br></pre></td></tr></table></figure>

<h3 id="Run-Queries"><a href="#Run-Queries" class="headerlink" title="Run Queries"></a>Run Queries</h3><p>Execute the following command to start to run queries. If you use <code>external</code> cache strategy, also need start plasma service manually as above.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> oap-benchmark-tool</span><br><span class="line">sh ./scripts/run_tpcds.sh</span><br></pre></td></tr></table></figure>

<p>When all the queries are done, you will see the <code>result.json</code> file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less.<br>And the Spark webUI OAP tab has more specific OAP cache metrics just as <a href="#use-dram-cache">section</a> step 5.</p>
<h2 id="Advanced-Configuration"><a href="#Advanced-Configuration" class="headerlink" title="Advanced Configuration"></a>Advanced Configuration</h2><ul>
<li><p><a href="./Advanced-Configuration.md#Additional-Cache-Strategies">Additional Cache Strategies</a>  </p>
<p>In addition to <strong>external</strong> cache strategy, SQL Data Source Cache also supports 3 other cache strategies: <strong>guava</strong>, <strong>noevict</strong>  and <strong>vmemcache</strong>.</p>
</li>
<li><p><a href="./Advanced-Configuration.md#Index-and-Data-Cache-Separation">Index and Data Cache Separation</a> </p>
<p>To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem.</p>
</li>
<li><p><a href="./Advanced-Configuration.md#Cache-Hot-Tables">Cache Hot Tables</a> </p>
<p>Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables.</p>
</li>
<li><p><a href="./Advanced-Configuration.md#Column-Vector-Cache">Column Vector Cache</a> </p>
<p>This document above uses <strong>binary</strong> cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time.</p>
</li>
<li><p><a href="./Advanced-Configuration.md#Large-Scale-and-Heterogeneous-Cluster-Support">Large Scale and Heterogeneous Cluster Support</a> </p>
<p>Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters.</p>
</li>
</ul>
<p>For more information and configuration details, please refer to <a href="Advanced-Configuration.md">Advanced Configuration</a>.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://hongw2019.github.io/OAP-1.0.0/2020/12/17/User-Guide/" data-id="ckisy2kky0002nfpvddt2fw50" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/OAP-1.0.0/archives/2020/12/">December 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/OAP-1.0.0/2020/12/17/Developer-Guide/">(no title)</a>
          </li>
        
          <li>
            <a href="/OAP-1.0.0/2020/12/17/Advanced-Configuration/">(no title)</a>
          </li>
        
          <li>
            <a href="/OAP-1.0.0/2020/12/17/User-Guide/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 Hexo<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/OAP-1.0.0/" class="mobile-nav-link">Home</a>
  
    <a href="/OAP-1.0.0/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/OAP-1.0.0/js/jquery-3.4.1.min.js"></script>



  
<script src="/OAP-1.0.0/fancybox/jquery.fancybox.min.js"></script>




<script src="/OAP-1.0.0/js/script.js"></script>





  </div>
</body>
</html>