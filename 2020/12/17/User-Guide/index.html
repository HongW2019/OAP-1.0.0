<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>User Guide | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="User Guide Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Adva">
<meta property="og:type" content="article">
<meta property="og:title" content="User Guide">
<meta property="og:url" content="https://hongw2019.github.io/OAP-1.0.0/2020/12/17/User-Guide/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="User Guide Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Adva">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hongw2019.github.io/OAP-1.0.0/2020/12/17/User-Guide/image/spark_shell_oap.png">
<meta property="og:image" content="https://hongw2019.github.io/OAP-1.0.0/2020/12/17/User-Guide/image/webUI.png">
<meta property="article:published_time" content="2020-12-17T10:30:46.066Z">
<meta property="article:modified_time" content="2020-12-17T10:30:46.041Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hongw2019.github.io/OAP-1.0.0/2020/12/17/User-Guide/image/spark_shell_oap.png">
  
    <link rel="alternate" href="/OAP-1.0.0/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/OAP-1.0.0/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/OAP-1.0.0/css/style.css">

  
    
<link rel="stylesheet" href="/OAP-1.0.0/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/OAP-1.0.0/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/OAP-1.0.0/">Home</a>
        
          <a class="main-nav-link" href="/OAP-1.0.0/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/OAP-1.0.0/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://hongw2019.github.io/OAP-1.0.0"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-User-Guide" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/OAP-1.0.0/2020/12/17/User-Guide/" class="article-date">
  <time class="dt-published" datetime="2020-12-17T10:30:46.066Z" itemprop="datePublished">2020-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      User Guide
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="User-Guide"><a href="#User-Guide" class="headerlink" title="User Guide"></a>User Guide</h1><ul>
<li><a href="#Prerequisites">Prerequisites</a></li>
<li><a href="#Getting-Started">Getting Started</a></li>
<li><a href="#Configuration-for-YARN-Cluster-Mode">Configuration for YARN Cluster Mode</a></li>
<li><a href="#Configuration-for-Spark-Standalone-Mode">Configuration for Spark Standalone Mode</a></li>
<li><a href="#Working-with-SQL-Index">Working with SQL Index</a></li>
<li><a href="#Working-with-SQL-Data-Source-Cache">Working with SQL Data Source Cache</a></li>
<li><a href="#Run-TPC-DS-Benchmark">Run TPC-DS Benchmark</a></li>
<li><a href="#Advanced-Configuration">Advanced Configuration</a></li>
</ul>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support.</p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h3 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h3><p>We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow <a href="../../../docs/OAP-Installation-Guide.md">OAP-Installation-Guide</a> and you can find compiled OAP jars under<br> <code>$HOME/miniconda2/envs/oapenv/oap_jars</code> once finished the installation.</p>
<p>If you’d like to build from source code, please refer to <a href="Developer-Guide.md">Developer Guide</a> for the detailed steps.</p>
<h3 id="Spark-Configurations"><a href="#Spark-Configurations" class="headerlink" title="Spark Configurations"></a>Spark Configurations</h3><p>Users usually test and run Spark SQL or Scala scripts in Spark Shell,  which launches Spark applications on YRAN with <strong><em>client</em></strong> mode. In this section, we will start with Spark Shell then introduce other use scenarios. </p>
<p>Before you run <code>$SPARK_HOME/bin/spark-shell </code>, you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file <code>$SPARK_HOME/conf/spark-defaults.conf</code> on your working node.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.extensions              org.apache.spark.sql.OapExtensions</span><br><span class="line"># absolute path of the jar on your working node</span><br><span class="line">spark.files                       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar,$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># relative path of the jar</span><br><span class="line">spark.executor.extraClassPath     .&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:.&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># absolute path of the jar on your working node</span><br><span class="line">spark.driver.extraClassPath       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br></pre></td></tr></table></figure>
<h3 id="Verify-Integration"><a href="#Verify-Integration" class="headerlink" title="Verify Integration"></a>Verify Integration</h3><p>After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell.</p>
<ol>
<li><p>Create a test data path on your HDFS. <code>hdfs:///user/oap/</code> for example.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir &#x2F;user&#x2F;oap&#x2F;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>Launch Spark Shell using the following command on your working node.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME&#x2F;bin&#x2F;spark-shell</span><br></pre></td></tr></table></figure>
</li>
<li><p>Execute the following commands in Spark Shell to test OAP integration. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(s&quot;&quot;&quot;CREATE TABLE oap_test (a INT, b STRING)</span><br><span class="line">       USING parquet</span><br><span class="line">       OPTIONS (path &#39;hdfs:&#x2F;&#x2F;&#x2F;user&#x2F;oap&#x2F;&#39;)&quot;&quot;&quot;.stripMargin)</span><br><span class="line">&gt; val data &#x3D; (1 to 30000).map &#123; i &#x3D;&gt; (i, s&quot;this is test $i&quot;) &#125;.toDF().createOrReplaceTempView(&quot;t&quot;)</span><br><span class="line">&gt; spark.sql(&quot;insert overwrite table oap_test select * from t&quot;)</span><br><span class="line">&gt; spark.sql(&quot;create oindex index1 on oap_test (a)&quot;)</span><br><span class="line">&gt; spark.sql(&quot;show oindex from oap_test&quot;).show()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>This test creates an index for a table and then shows it. If there are no errors, the OAP <code>.jar</code> is working with the configuration. The picture below is an example of a successfully run.</p>
<p><img src="./image/spark_shell_oap.png" alt="Spark_shell_running_results"></p>
<h2 id="Configuration-for-YARN-Cluster-Mode"><a href="#Configuration-for-YARN-Cluster-Mode" class="headerlink" title="Configuration for YARN Cluster Mode"></a>Configuration for YARN Cluster Mode</h2><p>Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in <strong><em>client</em></strong> mode. While Spark Submit tool can run Spark application in <strong><em>client</em></strong> or <strong><em>cluster</em></strong> mode, which is decided by <code>--deploy-mode</code> parameter. <a href="#Getting-Started">Getting Started</a> session has shown the configurations needed for <strong><em>client</em></strong> mode. If you are running Spark Submit tool in <strong><em>cluster</em></strong> mode, you need to follow the below configuration steps instead.</p>
<p>Add the following OAP configuration settings to <code>$SPARK_HOME/conf/spark-defaults.conf</code> on your working node before running <code>spark-submit</code> in <strong><em>cluster</em></strong> mode.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.extensions              org.apache.spark.sql.OapExtensions</span><br><span class="line"># absolute path on your working node</span><br><span class="line">spark.files                       $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar,$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># relative path    </span><br><span class="line">spark.executor.extraClassPath     .&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:.&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># relative path </span><br><span class="line">spark.driver.extraClassPath       .&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:.&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br></pre></td></tr></table></figure>

<h2 id="Configuration-for-Spark-Standalone-Mode"><a href="#Configuration-for-Spark-Standalone-Mode" class="headerlink" title="Configuration for Spark Standalone Mode"></a>Configuration for Spark Standalone Mode</h2><p>In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode:</p>
<ol>
<li>Make sure the OAP <code>.jar</code> at the same path of <strong>all</strong> the worker nodes.</li>
<li>Add the following configuration settings to <code>$SPARK_HOME/conf/spark-defaults.conf</code> on the working node.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.extensions               org.apache.spark.sql.OapExtensions</span><br><span class="line"># absolute path on worker nodes</span><br><span class="line">spark.executor.extraClassPath      $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br><span class="line"># absolute path on worker nodes</span><br><span class="line">spark.driver.extraClassPath        $HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-cache-&lt;version&gt;-with-spark-&lt;version&gt;.jar:$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;oap_jars&#x2F;oap-common-&lt;version&gt;-with-spark-&lt;version&gt;.jar</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Working-with-SQL-Index"><a href="#Working-with-SQL-Index" class="headerlink" title="Working with SQL Index"></a>Working with SQL Index</h2><p>After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include <code>index create</code>, <code>drop</code>, <code>refresh</code>, and <code>show</code>. Test these functions using the following examples in Spark Shell.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(s&quot;&quot;&quot;CREATE TABLE oap_test (a INT, b STRING)</span><br><span class="line">       USING parquet</span><br><span class="line">       OPTIONS (path &#39;hdfs:&#x2F;&#x2F;&#x2F;user&#x2F;oap&#x2F;&#39;)&quot;&quot;&quot;.stripMargin)</span><br><span class="line">&gt; val data &#x3D; (1 to 30000).map &#123; i &#x3D;&gt; (i, s&quot;this is test $i&quot;) &#125;.toDF().createOrReplaceTempView(&quot;t&quot;)</span><br><span class="line">&gt; spark.sql(&quot;insert overwrite table oap_test select * from t&quot;)       </span><br></pre></td></tr></table></figure>

<h3 id="Index-Creation"><a href="#Index-Creation" class="headerlink" title="Index Creation"></a>Index Creation</h3><p>Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP]</span><br></pre></td></tr></table></figure>
<p>The following example creates a B+ Tree index on column “a” of the <code>oap_test</code> table.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(&quot;create oindex index1 on oap_test (a)&quot;)</span><br></pre></td></tr></table></figure>
<p>Use SHOW OINDEX command to show all the created indexes on a specified table.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(&quot;show oindex from oap_test&quot;).show()</span><br></pre></td></tr></table></figure>
<h3 id="Use-Index"><a href="#Use-Index" class="headerlink" title="Use Index"></a>Use Index</h3><p>Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column “a”.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(&quot;SELECT * FROM oap_test WHERE a &#x3D; 1&quot;).show()</span><br></pre></td></tr></table></figure>

<h3 id="Drop-index"><a href="#Drop-index" class="headerlink" title="Drop index"></a>Drop index</h3><p>Use DROP OINDEX command to drop a named index.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.sql(&quot;drop oindex index1 on oap_test&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="Working-with-SQL-Data-Source-Cache"><a href="#Working-with-SQL-Data-Source-Cache" class="headerlink" title="Working with SQL Data Source Cache"></a>Working with SQL Data Source Cache</h2><p>Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware.</p>
<h3 id="Use-DRAM-Cache"><a href="#Use-DRAM-Cache" class="headerlink" title="Use DRAM Cache"></a>Use DRAM Cache</h3><ol>
<li><p>Make the following configuration changes in Spark configuration file <code>$SPARK_HOME/conf/spark-defaults.conf</code>. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark.memory.offHeap.enabled                      false</span><br><span class="line">spark.oap.cache.strategy                          guava</span><br><span class="line">spark.sql.oap.cache.memory.manager                offheap</span><br><span class="line"># according to the resource of cluster</span><br><span class="line">spark.executor.memoryOverhead                     50g</span><br><span class="line"># equal to the size of executor.memoryOverhead</span><br><span class="line">spark.executor.sql.oap.cache.offheap.memory.size  50g</span><br><span class="line"># for parquet fileformat, enable binary cache</span><br><span class="line">spark.sql.oap.parquet.binary.cache.enabled        true</span><br><span class="line"># for orc fileformat, enable binary cache</span><br><span class="line">spark.sql.oap.orc.binary.cache.enabled            true</span><br></pre></td></tr></table></figure>

<p><strong><em>NOTE</em></strong>: Change <code>spark.executor.sql.oap.cache.offheap.memory.size</code> based on the availability of DRAM capacity to cache data, and its size is equal to <code>spark.executor.memoryOverhead</code></p>
</li>
<li><p>Launch Spark <strong><em>ThriftServer</em></strong></p>
<p>Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. </p>
<p>The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the <a href="#Working-with-SQL-Index">Working with SQL Index</a> section, which creates the <code>oap_test</code> table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL’s through Beeline for creating your tables.</p>
<p>When you run <code>spark-shell</code> to create the <code>oap_test</code> table, <code>metastore_db</code> will be created in the directory where you ran ‘$SPARK_HOME/bin/spark-shell’. <strong><em>Go to that directory</em></strong> and execute the following command to launch Thrift JDBC server and run queries.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">. $SPARK_HOME&#x2F;sbin&#x2F;start-thriftserver.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">. $SPARK_HOME&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;&lt;mythriftserver&gt;:10000       </span><br></pre></td></tr></table></figure>

<p>After the connection is established, execute the following commands to check the metastore is initialized correctly.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; SHOW databases;</span><br><span class="line">&gt; USE default;</span><br><span class="line">&gt; SHOW tables;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Run queries on the table that will use the cache automatically. For example,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; SELECT * FROM oap_test WHERE a &#x3D; 1;</span><br><span class="line">&gt; SELECT * FROM oap_test WHERE a &#x3D; 2;</span><br><span class="line">&gt; SELECT * FROM oap_test WHERE a &#x3D; 3;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example.</p>
<p><img src="./image/webUI.png" alt="webUI"></p>
</li>
</ol>
<h3 id="Use-PMem-Cache"><a href="#Use-PMem-Cache" class="headerlink" title="Use PMem Cache"></a>Use PMem Cache</h3><h4 id="Prerequisites-1"><a href="#Prerequisites-1" class="headerlink" title="Prerequisites"></a>Prerequisites</h4><p>The following steps are required to configure OAP to use PMem cache with <code>external</code> cache strategy.</p>
<ul>
<li><p>PMem hardware is successfully deployed on each node in cluster.</p>
</li>
<li><p>SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries.  <a target="_blank" rel="noopener" href="http://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/">Plasma</a> is a high-performance shared-memory object store, it’s a component of <a target="_blank" rel="noopener" href="https://github.com/apache/arrow">Apache Arrow</a>. We have modified Plasma to support PMem, and open source on <a target="_blank" rel="noopener" href="https://github.com/Intel-bigdata/arrow/tree/branch-0.17.0-oap-1.0">Intel-bigdata Arrow</a> repo. Build and install step can refer to <a href="./Developer-Guide.md#Plasma-installation">Plasma installation</a>. If you have finished <a href="../../../docs/OAP-Installation-Guide.md">OAP Installation Guide</a>, Plasma will be automatically installed.</p>
</li>
<li><p>Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path.</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Socket Configuration -&gt; Memory Configuration -&gt; NGN Configuration -&gt; Snoopy mode for AD : Enabled</span><br><span class="line">Socket Configuration -&gt; Intel UPI General Configuration -&gt; Stale AtoS :  Disabled</span><br><span class="line">&#96;&#96;&#96; </span><br><span class="line"></span><br><span class="line">- It&#39;s strongly advised to use [Linux device mapper](https:&#x2F;&#x2F;pmem.io&#x2F;2018&#x2F;05&#x2F;15&#x2F;using_persistent_memory_devices_with_the_linux_device_mapper.html) to interleave PMem across sockets and get maximum size for Plasma.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>   // use ipmctl command to show topology and dimm info of PMem<br>   ipmctl show -topology<br>   ipmctl show -dimm<br>   // provision PMem in app direct mode<br>   ipmctl create -goal PersistentMemoryType=AppDirect<br>   // reboot system to make configuration take affect<br>   reboot<br>   // check capacity provisioned for app direct mode(AppDirectCapacity)<br>   ipmctl show -memoryresources<br>   // show the PMem region information<br>   ipmctl show -region<br>   // create namespace based on the region, multi namespaces can be created on a single region<br>   ndctl create-namespace -m fsdax -r region0<br>   ndctl create-namespace -m fsdax -r region1<br>   // show the created namespaces<br>   fdisk -l<br>   // create and mount file system<br>   sudo dmsetup create striped-pmem<br>   mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem<br>   mkdir -p /mnt/pmem<br>   mount -o dax /dev/mapper/striped-pmem /mnt/pmem<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">   </span><br><span class="line">   For more information you can refer to [Quick Start Guide: Provision Intel® Optane™ DC Persistent Memory](https:&#x2F;&#x2F;software.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;develop&#x2F;articles&#x2F;quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux.html)</span><br><span class="line"></span><br><span class="line">- Download &#96;arrow-plasma-0.17.0.jar&#96; from [Maven repository](https:&#x2F;&#x2F;repo1.maven.org&#x2F;maven2&#x2F;com&#x2F;intel&#x2F;arrow&#x2F;arrow-plasma&#x2F;0.17.0&#x2F;arrow-plasma-0.17.0.jar), then copy it to your ***$SPARK_HOME&#x2F;jars*** directory. Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### Configuration for NUMA</span><br><span class="line"></span><br><span class="line">Install &#96;numactl&#96; to bind the executor to the PMem device on the same NUMA node. </span><br><span class="line"></span><br><span class="line">   &#96;&#96;&#96;yum install numactl -y &#96;&#96;&#96;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### Configuration for enabling PMem cache</span><br><span class="line"></span><br><span class="line">Add the following configuration to &#96;$SPARK_HOME&#x2F;conf&#x2F;spark-defaults.conf&#96;.</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h1 id="2x-number-of-your-worker-nodes"><a href="#2x-number-of-your-worker-nodes" class="headerlink" title="2x number of your worker nodes"></a>2x number of your worker nodes</h1><p>spark.executor.instances          6</p>
<h1 id="enable-numa"><a href="#enable-numa" class="headerlink" title="enable numa"></a>enable numa</h1><p>spark.yarn.numa.enabled           true</p>
<h1 id="enable-SQL-Index-and-Data-Source-Cache-extension-in-Spark"><a href="#enable-SQL-Index-and-Data-Source-Cache-extension-in-Spark" class="headerlink" title="enable SQL Index and Data Source Cache extension in Spark"></a>enable SQL Index and Data Source Cache extension in Spark</h1><p>spark.sql.extensions              org.apache.spark.sql.OapExtensions</p>
<h1 id="absolute-path-of-the-jar-on-your-working-node-when-in-Yarn-client-mode"><a href="#absolute-path-of-the-jar-on-your-working-node-when-in-Yarn-client-mode" class="headerlink" title="absolute path of the jar on your working node, when in Yarn client mode"></a>absolute path of the jar on your working node, when in Yarn client mode</h1><p>spark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar</p>
<h1 id="relative-path-of-the-jar-when-in-Yarn-client-mode"><a href="#relative-path-of-the-jar-when-in-Yarn-client-mode" class="headerlink" title="relative path of the jar, when in Yarn client mode"></a>relative path of the jar, when in Yarn client mode</h1><p>spark.executor.extraClassPath     ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar</p>
<h1 id="absolute-path-of-the-jar-on-your-working-node-when-in-Yarn-client-mode-1"><a href="#absolute-path-of-the-jar-on-your-working-node-when-in-Yarn-client-mode-1" class="headerlink" title="absolute path of the jar on your working node,when in Yarn client mode"></a>absolute path of the jar on your working node,when in Yarn client mode</h1><p>spark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar</p>
<h1 id="for-parquet-file-format-enable-binary-cache"><a href="#for-parquet-file-format-enable-binary-cache" class="headerlink" title="for parquet file format, enable binary cache"></a>for parquet file format, enable binary cache</h1><p>spark.sql.oap.parquet.binary.cache.enabled                   true</p>
<h1 id="for-ORC-file-format-enable-binary-cache"><a href="#for-ORC-file-format-enable-binary-cache" class="headerlink" title="for ORC file format, enable binary cache"></a>for ORC file format, enable binary cache</h1><p>spark.sql.oap.orc.binary.cache.enabled                       true</p>
<h1 id="enable-external-cache-strategy"><a href="#enable-external-cache-strategy" class="headerlink" title="enable external cache strategy"></a>enable external cache strategy</h1><p>spark.oap.cache.strategy                                     external<br>spark.sql.oap.dcpmm.free.wait.threshold                      50000000000</p>
<h1 id="according-to-your-executor-core-number"><a href="#according-to-your-executor-core-number" class="headerlink" title="according to your executor core number"></a>according to your executor core number</h1><p>spark.executor.sql.oap.cache.external.client.pool.size       10</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Start Plasma service manually</span><br><span class="line"></span><br><span class="line">Plasma config parameters:  </span><br></pre></td></tr></table></figure>
<p> -m  how much Bytes share memory Plasma will use<br> -s  Unix Domain sockcet path<br> -d  PMem directory<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find &#96;plasma-store-server&#96; in the path **$HOME&#x2F;miniconda2&#x2F;envs&#x2F;oapenv&#x2F;bin&#x2F;**.</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Remember to kill &#96;plasma-store-server&#96; process if you no longer need cache, and you should delete &#96;&#x2F;tmp&#x2F;plasmaStore&#96; which is a Unix domain socket.  </span><br><span class="line">  </span><br><span class="line">- Use Yarn to start Plamsa service  </span><br><span class="line">When using Yarn(Hadoop version &gt;&#x3D; 3.1) to start Plasma service, you should provide a json file as below.</span><br></pre></td></tr></table></figure>
<p>{<br>  “name”: “plasma-store-service”,<br>  “version”: 1,<br>  “components” :<br>  [<br>   {<br>     “name”: “plasma-store-service”,<br>     “number_of_containers”: 3,<br>     “launch_command”: “plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem”,<br>     “resource”: {<br>       “cpus”: 1,<br>       “memory”: 512<br>     }<br>   }<br>  ]<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Run command  &#96;&#96;&#96;yarn app -launch plasma-store-service &#x2F;tmp&#x2F;plasmaLaunch.json&#96;&#96;&#96; to start Plasma server.  </span><br><span class="line">Run &#96;&#96;&#96;yarn app -stop plasma-store-service&#96;&#96;&#96; to stop it.  </span><br><span class="line">Run &#96;&#96;&#96;yarn app -destroy plasma-store-service&#96;&#96;&#96;to destroy it.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Verify PMem cache functionality</span><br><span class="line"></span><br><span class="line">- After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the [Use DRAM Cache](#use-dram-cache) guide to verify that cache is working correctly.</span><br><span class="line"></span><br><span class="line">- Check PMem cache size by checking disk space with &#96;df -h&#96;.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Run TPC-DS Benchmark</span><br><span class="line"></span><br><span class="line">This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I&#x2F;O intensive queries to simplify performance evaluation.</span><br><span class="line"></span><br><span class="line">We created some tool scripts [oap-benchmark-tool.zip](https:&#x2F;&#x2F;github.com&#x2F;Intel-bigdata&#x2F;OAP&#x2F;releases&#x2F;download&#x2F;v0.9.0-spark-3.0.0&#x2F;oap-benchmark-tool.zip) to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly.</span><br><span class="line"></span><br><span class="line">### Prerequisites</span><br><span class="line"></span><br><span class="line">- Python 2.7+ is required on the working node. </span><br><span class="line"></span><br><span class="line">### Prepare the Tool</span><br><span class="line"></span><br><span class="line">1. Download [oap-benchmark-tool.zip](https:&#x2F;&#x2F;github.com&#x2F;Intel-bigdata&#x2F;OAP&#x2F;releases&#x2F;download&#x2F;v0.9.0-spark-3.0.0&#x2F;oap-benchmark-tool.zip) and unzip to a folder (for example, &#96;oap-benchmark-tool&#96; folder) on your working node. </span><br><span class="line">2. Copy &#96;oap-benchmark-tool&#x2F;tools&#x2F;tpcds-kits&#96; to ***ALL*** worker nodes under the same folder (for example, &#96;&#x2F;home&#x2F;oap&#x2F;tpcds-kits&#96;).</span><br><span class="line"></span><br><span class="line">### Generate TPC-DS Data</span><br><span class="line"></span><br><span class="line">1. Update the values for the following variables in &#96;oap-benchmark-tool&#x2F;scripts&#x2F;tool.conf&#96; based on your environment and needs.</span><br><span class="line"></span><br><span class="line">   - SPARK_HOME: Point to the Spark home directory of your Spark setup.</span><br><span class="line">   - TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, &#x2F;home&#x2F;oap&#x2F;tpcds-kits</span><br><span class="line">   - NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port.</span><br><span class="line">   - THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server.</span><br><span class="line">   - DATA_SCALE: The data scale to be generated in GB</span><br><span class="line">   - DATA_FORMAT: The data file format. You can specify parquet or orc</span><br><span class="line"></span><br><span class="line">   For example:</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>export SPARK_HOME=/home/oap/spark-3.0.0<br>export TPCDS_KITS_DIR=/home/oap/tpcds-kits<br>export NAMENODE_ADDRESS=mynamenode:9000<br>export THRIFT_SERVER_ADDRESS=mythriftserver<br>export DATA_SCALE=1024<br>export DATA_FORMAT=parquet</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. Start data generation.</span><br><span class="line"></span><br><span class="line">   In the root directory of this tool (&#96;oap-benchmark-tool&#96;), run &#96;scripts&#x2F;run_gen_data.sh&#96; to start the data generation process. </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>cd oap-benchmark-tool<br>sh ./scripts/run_gen_data.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   Once finished, the &#96;$scale&#96; data will be generated in the HDFS folder &#96;genData$scale&#96;. And a database called &#96;tpcds_$format$scale&#96; will contain the TPC-DS tables.</span><br><span class="line"></span><br><span class="line">### Start Spark Thrift Server</span><br><span class="line"></span><br><span class="line">Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server.</span><br><span class="line"></span><br><span class="line">#### Use PMem as Cache Media</span><br><span class="line"></span><br><span class="line">Update the configuration values in &#96;scripts&#x2F;spark_thrift_server_yarn_with_PMem.sh&#96; to reflect your environment. </span><br><span class="line">Normally, you need to update the following configuration values to cache to PMem.</span><br><span class="line"></span><br><span class="line">- --driver-memory</span><br><span class="line">- --executor-memory</span><br><span class="line">- --executor-cores</span><br><span class="line">- --conf spark.oap.cache.strategy</span><br><span class="line">- --conf spark.sql.oap.dcpmm.free.wait.threshold</span><br><span class="line">- --conf spark.executor.sql.oap.cache.external.client.pool.size</span><br><span class="line"></span><br><span class="line">These settings will override the values specified in Spark configuration file ( &#96;spark-defaults.conf&#96;). After the configuration is done, you can execute the following command to start Thrift Server.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>cd oap-benchmark-tool<br>sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In this script, we use &#96;vmem&#96; as cache strategy for Parquet Binary data cache. </span><br><span class="line"></span><br><span class="line">#### Use DRAM as Cache Media </span><br><span class="line"></span><br><span class="line">Update the configuration values in &#96;scripts&#x2F;spark_thrift_server_yarn_with_DRAM.sh&#96; to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM.</span><br><span class="line"></span><br><span class="line">- --driver-memory</span><br><span class="line">- --executor-memory</span><br><span class="line">- --executor-cores</span><br><span class="line">- --conf spark.executor.sql.oap.cache.offheap.memory.size</span><br><span class="line">- --conf spark.executor.memoryOverhead</span><br><span class="line"></span><br><span class="line">These settings will override the values specified in Spark configuration file (&#96;spark-defaults.conf&#96;). After the configuration is done, you can execute the following command to start Thrift Server.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>cd oap-benchmark-tool<br>sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh  start</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### Run Queries</span><br><span class="line"></span><br><span class="line">Execute the following command to start to run queries.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>cd oap-benchmark-tool<br>sh ./scripts/run_tpcds.sh</p>
<pre><code>
When all the queries are done, you will see the `result.json` file in the current directory.

## Advanced Configuration

- [Additional Cache Strategies](./Advanced-Configuration.md#Additional-Cache-Strategies)  

  In addition to **external** cache strategy, SQL Data Source Cache also supports 3 other cache strategies: **guava**, **noevict**  and **vmemcache**.
- [Index and Data Cache Separation](./Advanced-Configuration.md#Index-and-Data-Cache-Separation) 

  To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem.
- [Cache Hot Tables](./Advanced-Configuration.md#Cache-Hot-Tables) 

  Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables.
- [Column Vector Cache](./Advanced-Configuration.md#Column-Vector-Cache) 

  This document above uses **binary** cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time.
- [Large Scale and Heterogeneous Cluster Support](./Advanced-Configuration.md#Large-Scale-and-Heterogeneous-Cluster-Support) 

  Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters.

For more information and configuration details, please refer to [Advanced Configuration](Advanced-Configuration.md).</code></pre>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://hongw2019.github.io/OAP-1.0.0/2020/12/17/User-Guide/" data-id="ckiswttsk000027pv7xw4c7k6" data-title="User Guide" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/OAP-1.0.0/archives/2020/12/">December 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/OAP-1.0.0/2020/12/17/User-Guide/">User Guide</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 Hexo<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/OAP-1.0.0/" class="mobile-nav-link">Home</a>
  
    <a href="/OAP-1.0.0/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/OAP-1.0.0/js/jquery-3.4.1.min.js"></script>



  
<script src="/OAP-1.0.0/fancybox/jquery.fancybox.min.js"></script>




<script src="/OAP-1.0.0/js/script.js"></script>





  </div>
</body>
</html>